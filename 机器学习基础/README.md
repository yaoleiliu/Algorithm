[TOC]
# 一、评估指标
## 1.1 定义
**准确率**：分类正确的样本占总样本个数的比例</br>
**精确率**：预测为正的样本中有多少比例是真正的正样本</br>
```math
P = \frac{TP}{TP + FP}  
```
**召回率**：样本中正例的部分，有多大比例被预测为正例了</br>
```math
P = \frac{TP}{TP + FN}  
```
*上面两个概念其实就是分母的不同，一个是预测为正的样本数，一个是原来样本中所有的正样本数*

**P-R曲线**</br>
</br>
**定义**：横轴是召回率，纵轴是精确率。其上的一个点代表在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率</br>
</br>
**绘制过程**：将阈值从高到低移动，返回不同的精确率和召回率绘制生成</br>
</br>
**ROC曲线**

* 假阳性率

```math
FPR = \frac{FP}{N}
```

N是真实的负样本数量，FP是N个负样本中被分类器预测为正样本的个数</br>

* 真阳性率

```math
TPR = \frac{TP}{P}
```

P是真实的正样本数量，TP是P个正样本中被分类器预测为正样本的个数

**ROC曲线定义**：横坐标为假阳性率、纵坐标为真阳性率的曲线</br>
</br>
**ROC曲线绘制**：通过不断移动阈值(截断点)来获取一组关键点，进而进行曲线绘制</br>
</br>
**AUC**：指ROC曲线下的面积大小，该值能够量化地反映基于ROC曲线衡量出的模型性能。计算AUC值只需要沿着ROC横轴做积分就可以了，它的取值在0.5~1之间。AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好</br>
</br>
*相比PR曲线，ROC曲线有一个特点，当正负样本的分布发生变化时，ROC曲线的形状能够基本保持不变，而PR曲线的形状会发生较剧烈的变化*</br>

### 其它评价指标
- mae 

```math
mae = \frac{1}{m} \sum_{i=1}^m |(y_{i} - \hat y_{i})|
```
- mape

```math
mape = \frac{1}{n} \sum_{1}^{n}|\frac{y_{i} - \hat y_{i}}{y_{i}}|
```
- rmse  

```math
rmse = \sqrt {\frac{1}{m} \sum_{i=1}^m (y_{i} - \hat y_{i})^2}
```

## 1.2 A/B测试
为什幺进行A/B测试？</br>
1）离线评估无法完全消除模型过拟合的影响，因此，得出的离线评估结果无法完全替代线上评估结果</br>
2）离线评估无法完全还原线上的工程环境</br>
3）线上系统的某些商业指标在离线评估中无法计算。离线评估一般针对模型本身进行评估，而与模型相关的其他指标，特别是商业指标，往往无法获取</br>
</br>
如何进行线上A/B测试？</br>
进行A/B测试的主要手段是对用户进行分桶，即将用户分成实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以旧模型

## 1.3 过拟合与欠拟合
过拟合是指模型对于训练数据拟合呈过当的情况，反映到评估指标上，就是模型在训练集上表现很好，但在测试集和新数据上的表现较差。欠拟合指的是模型在训练集和测试集的表现都不好的情况。</br>

*降低“过拟合”的方法？*

- 从数据入手，获得更多的训练数据
- 降低模型复杂度。神经网络：减少网络层数、神经元个数等；决策树：降低树的深度、剪枝等
- 正则化方法。L1、L2
- 集成学习方法。把多个模型集成在一起，降低单一模型的过拟合风险，如Bagging方法

*降低“欠拟合”的方法？*

- 添加新特征
- 增加模型复杂度
- 减小正则化系数  

# 二、传统机器学习算法
## 2.1 决策树
### 信息增益
设X是一个取有限个值的离散随机变量，概率分布为：

```math
P(X=x_{i}) = p_{i}
```
则随机变量的熵定义为：

```math
H(X) = - \sum_{i=1}^n p_{i} logp_{i}
```

熵越大，随机变量的不确定性就越大。</br>
</br>
设有随机变量(X, Y)，其联合概率分布为：

```math
P(X=x_{i}, Y=y_{i}) = p_{ij}
```

条件熵H(Y|X)表示已知随机变量X的条件下随机变量Y的不确定性，定义为X给定条件下Y的条件概率分布的熵对X的数学期望:

```math
H(Y|X) = \sum_{i=1}^n p_{i}H(Y|X=x_{i})
```

信息增益定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差：

```math
g(D|A) = H(D) - H(D|A)
```

信息增益表示由于特征A而使得对数据集D的分类的不确定性减少的程度，信息增益大的特征具有更强的分类能力</br>
</br>
以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正：

```math
g_{R}(D, A) = \frac{g(D, A)}{H_{A}(D)} 
```

其中：

```math
H_{A}(D) = -\sum_{i=1}^{n} \frac{|D_{i}|}{|D|} log_{2} \frac{|D_{i}|}{|D|}
```
n是特征A取值的个数

### 基尼指数
分类问题中，假设有K个类，样本点属于第k类的概率为p_{k}，则概率分布的基尼指数为：

```math
Gini(p) = \sum_{k=1}^K p_{k}(1-p_{k}) = 1 - \sum_{k=1}^{K}p_{k}^2
```

对于给定的样本集合D，其基尼指数为：

```math
Gini(D) = 1 - \sum_{k=1}^{K} (\frac{|C_{k}|}{|D|})^2
```

如果样本集合D根据特征A是否取某一可能值而被分割成两部分，即：

```math
D_{1} = \{ (x, y) \subseteq D|A(X=a) \}, D_{2} = D - D_{1}
```

则在特征A的条件下，集合D的基尼指数定义为：

```math
Gini(D, A) = \frac{|D_{1}|}{|D|} Gini(D_{1}) + \frac{|D_{2}|}{|D|}Gini(D_{2})
```
基尼指数越大，样本集合的不确定性就越大。在选择特征时，应选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点

### 经典决策树

决策树 | 特征选择算法
---|---
ID3 | 信息增益最大原则
C4.5 | 信息增益比最大原则
CART回归树 | 平方误差最小原则
CART分类树 | 基尼指数最小原则

## 2.2 逻辑斯蒂回归
### 逻辑斯蒂回归模型公式定义
逻辑斯蒂回归模型：

```math
P(Y=1|X) = \frac{e^{w*x+b}}{1+e^{w*x+b}}
```

</br>

```math
P(Y=0|X) = \frac{1}{1+e^{w*x+b}}
```

对于给定的输入实例x，按照上面两个公式可以求得P(Y=1|X)和P(Y=0|X)。逻辑斯蒂回归会比较两个条件概率值的大小，将实例x分到概率值较大的那一类。</br>
</br>
一个事件的几率是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是p，则该事件的对数几率是：

```math
logit(p) = log \frac{p}{1-p}
```

对于逻辑斯蒂回归而言：

```math
log \frac{P(Y=1|X)}{1-P(Y=1|X)} = w * x
```

这就是说，输出Y=1的对数几率是由输入x的线性函数表示的模型，即逻辑斯蒂回归模型

### 模型参数估计
应用极大似然估计法估计模型参数，设：

```math

P(Y=1|x) = \pi(x), P(Y=0|x) = 1-\pi(x)
```
似然函数为：

```math
\prod_{i=1}^{N}[\pi(x_{i})]^{y_{i}}[1-\pi(x_{i})]^{1-y_{i}}
```

对数似然函数为：

```math
\begin{aligned}
L(w) &= \sum_{i=1}^{N}[y_{i}log\pi(x_{i}) + (1-y_{i})log(1-\pi(x_{i}))] \\
     &= \sum_{i=1}^{N}[y_{i}log\frac{\pi(x_{i})}{1-\pi(x_{i})} + log(1-\pi(x_{i}))] \\
     &= \sum_{i=1}^{N}[y_{i}(w*x_{i}) - log(1+e^{w*x_{i}})]
\end{aligned}
```

问题变成了以对数似然函数为目标函数的最优化问题，逻辑斯蒂回归学习中通常采用的方法是梯度下降法和拟牛顿法

### 相关拓展
*逻辑斯蒂回归相比于线性回归，有何异同？* </br>
</br>
相同点：
- 二者都是用了极大似然估计来对训练样本进行建模
- 二者在求解超参数的过程中，都可以使用梯度下降方法

</br>
不同点：

- 逻辑斯蒂回归处理的是分类问题，线性回归处理的是回归问题，这是两者最本质的区别
- 逻辑回归中的因变量是离散的，而线性回归中的因变量是连续的

</br>

*逻辑斯蒂回归如何处理多分类问题？* </br>
1）一个样本只属于一个标签</br>
此时可以假设每个样本属于不同标签的概率服从几何分布，使用softmax(多项逻辑回归)来进行分类：

```math
h_{\theta} = \left[ \begin{matrix} p(y=1|x;\theta)\\
                                   p(y=2|x;\theta)\\
                                   ...\\
                                   p(y=k|x;\theta) \end{matrix} \right] 
           = \frac{1}{\sum_{j=1}^{k}e^{\theta_{j}^{T}x}}
             \left[ \begin{matrix} e^{\theta_{1}^{T}x}\\
                                   e^{\theta_{2}^{T}x}\\
                                   ...\\
                                   e^{\theta_{k}^{T}x} \end{matrix} \right] 
```
特别地，当类别数为2时：

```math
h_{\theta} = \frac{1}{e^{\theta_{1}^{T}x} + e^{\theta_{2}^{T}x}}
             \left[ \begin{matrix} e^{\theta_{1}^{T}x}\\
                                   e^{\theta_{2}^{T}x}\end{matrix} \right] 
```
多项逻辑回归具有参数冗余的特点，即将参数同时加减一个向量后预测结果保持不变。上面式子可变换为：

```math
\begin{aligned}
h_{\theta} &= \frac{1}{e^{0*x} + e^{(\theta_{2}^{T}-\theta_{1}^{T})x}}
             \left[ \begin{matrix} e^{0*x}\\
                                   e^{(\theta_{2}^{T}-\theta_{1}^{T})x}\end{matrix} \right] \\
           &= \left[ \begin{matrix} \frac{1}{1+e^{\theta^{T}x}}\\
                                   1-\frac{1}{1+e^{\theta^{T}x}}\end{matrix} \right]
\end{aligned}
```
整理后的式子与逻辑回归保持一致。因此，多项逻辑回归实际上是二分类逻辑回归在多标签分类下的一种扩展</br>
</br>
2）一个样本可能属于多个标签</br>
当存在样本可能属于多个标签的情况时，可以训练k个二分类逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类，训练分类器时，需要把标签重新整理为“第i类标签”和“非第i类标签”两大类

## 2.3 支持向量机
### 函数间隔与几何间隔
- 函数间隔

对于给定的训练数据集T和超平面(w, b)，定义超平面(w, b)关于某一样本点的函数间隔为：

```math
\hat \gamma_{i} =  y_{i}(w*x_{i}+b) 
```

则超平面(w, b)关于训练集T的函数间隔为超平面(w, b)关于T中所有样本点的函数间隔最小值，即：

```math
\hat \gamma = min \hat \gamma_{i}
```

函数间隔可以表示分类预测的正确性和确信度，但只要成比例地改变w和b，超平面没有改变，函数间隔却变为原来的2倍。因此，有必要对超平面的法向量w加一些约束，如||w||=1，使得间隔是确定的，这时，函数间隔变为几何间隔。

- 几何间隔

对于给定的训练数据集T和超平面(w, b)，定义超平面(w, b)关于某一样本点的几何间隔为：

```math
\gamma_{i} = y_{i}(\frac{w}{||w||}x_{i} + \frac{b}{||w||})
```

则超平面(w, b)关于训练集T的几何间隔为超平面(w, b)关于T中所有样本点的几何间隔最小值，即：

```math
\hat \gamma = min \hat \gamma_{i}
```

### 支持向量和间隔边界
在线性可分的情况下，训练数据集中与分离超平面距离最近的样本点实例称为支持向量：

```math
y_{i}(w*x_{i}+b) - 1 = 0
```

对于正例点，支持向量在超平面：

```math
H1:w*x+b=1
```

对于负例点，支持向量在超平面：

```math
H1:w*x+b=-1
```


`$H_{1}$`和`$H_{2}$`之间的距离称为间隔，间隔依赖于分离超平面的法向量w，等于`$\frac{2}{||w||}$`。`$H_{1}$`和`$H_{2}$`称为间隔边界

### 支持向量机学习算法
支持向量机学习的基本想法是求解能够正确划分训练数据集并且**几何间隔**最大的分离超平面，用数学公式表示：

```math
\begin{array}{l}
max　\gamma \\
s.t.　y_{i}(\frac{w}{||w||}*x_{i} + \frac{b}{||w||})>=\gamma
\end{array}
```
由函数间隔和几何间隔的关系，可将上述公式转换为：

```math
\begin{array}{l}
max　\frac{\hat \gamma}{||w||} \\
s.t.　y_{i}(w*x_{i}+b)>=\hat \gamma
\end{array}
```

w和b成比例缩放对上述优化问题不会产生任何影响，因此，可取`$\hat \gamma = 1$`；另外，最小化`$\frac{1}{||w||}$`和最 小化`$\frac{1}{2}||w||^{2}$`是等价的，因此，可转换为下面的最优化问题：

```math
\begin{array}{l}
min　\frac{1}{2}||w||^{2} \\
s.t.　y_{i}(w*x{i}+b)-1>=0　　　　　　　　原始问题
\end{array}
```

应用**拉格朗日对偶性**，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法。这样做的优点是：1）一是对偶问题往往更容易求解；2）二是自然引入核函数，进而推广到非线性分类问题。</br>
</br>
首先构建拉格朗日函数：

```math
L(w, b, \alpha) = \frac{1}{2}||w|| - \sum_{i=1}^{N}\alpha_{i}y_{i}(w*x_{i}+b) + \sum_{i=1}^{N}\alpha_{i}
```

原始问题的对偶问题是极大极小问题：

```math
max_{\alpha}min_{w,b}L(w,b,\alpha)
```

为了得到对偶问题的解，需要先求得`$L(w, b, \alpha)$`对w,b的极小，再求得对`$\alpha$`的极大

- 求`$min_{w,b}L(w, b, \alpha)$`

将拉格朗日函数`$L(w, b, \alpha)$`分别对w,b求偏导数并令其等于0</br>

```math
\begin{array}{l}
\Delta_{w}L(w, b, \alpha) = w - \sum_{i=1}^{N}\alpha_{i}y_{y}x_{i}=0 \\
\Delta_{b}L(w, b, \alpha) = \sum_{i=1}^{N}\alpha_{i}y_{i}=0
\end{array}
```

可求得：

```math
W = \sum_{i=1}^{N}\alpha_{i}y_{i}x_{i}　　　　\sum_{i=1}^{N}\alpha_{i}y_{i}=0
```

结合拉格朗日函数可得：

```math
\begin{aligned}
L(w, b, \alpha) &= \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}*x_{j}) - \sum_{i=1}^{N}\alpha_{i}y_{i}((\sum_{j=1}^{N}\alpha_{j}y_{j}x_{j})*x_{i} + b) + \sum_{i=1}^{N}\alpha_{i} \\
&= -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}*x_{j}) + \sum_{i=1}^{N}\alpha_{i}
\end{aligned}
```

- 求`$min_{w,b}L(w,b,\alpha)$`对`$\alpha$`的极大，即是对偶问题 

```math
\begin{array}{l}
max_{\alpha}　　　-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}*x_{j}) + \sum_{i=1}^{N}\alpha_{i} \\
\\
s.t.　　　　\sum_{i=1}^{N}\alpha_{i}y_{i} = 0
\end{array}
```

将上面目标函数由求极大转换成求极小，可得到下面与之等价的对偶最优化问题：

```math
\begin{array}{l}
min_{\alpha}　　　\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}*x_{j}) - \sum_{i=1}^{N}\alpha_{i} \\
\\
s.t.　　　　\sum_{i=1}^{N}\alpha_{i}y_{i}=0
\end{array}
```
